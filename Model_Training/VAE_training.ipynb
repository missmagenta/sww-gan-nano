{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\waren\\mambaforge\\envs\\cc\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import urllib.parse\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "import torch\n",
    "import matplotlib as plt\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path_to_data, batch_size=32, crop_size=224, num_of_channels=1):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomRotation(degrees=(0, 360)),\n",
    "            transforms.Resize(crop_size),\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Grayscale(num_output_channels=num_of_channels),\n",
    "        ]\n",
    "    )\n",
    "    dataset = ImageFolder(root=path_to_data, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_height: int,\n",
    "        enc_type: str = \"default\",\n",
    "        first_conv: bool = False,\n",
    "        maxpool1: bool = False,\n",
    "        enc_out_dim: int = 5488,\n",
    "        kl_coeff: float = 0.01,\n",
    "        latent_dim: int = 5488,\n",
    "        lr: float = 1e-3,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "        self.kl_coeff = kl_coeff\n",
    "        self.enc_out_dim = enc_out_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_height = input_height\n",
    "\n",
    "        if enc_type == \"default\":\n",
    "            if input_height == 224:\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Conv2d(1, 224, (3,3), stride=(1,1), padding=(1,1)),  # 224 x 224 x 224\n",
    "                    nn.ReLU(True),\n",
    "                    nn.MaxPool2d(2),  # 224 x 112 x 112\n",
    "        \n",
    "                    nn.Conv2d(224, 112, (3,3), stride=(1,1), padding=(1,1)),  # 112 x 112 x 112\n",
    "                    nn.ReLU(True),\n",
    "                    nn.MaxPool2d(2),  # 112 x 56 x 56\n",
    "        \n",
    "                    nn.Conv2d(112, 56, (3,3), stride=(1,1), padding=(1,1)),  # 56 x 56 x 56\n",
    "                    nn.ReLU(True),\n",
    "                    nn.MaxPool2d(2),  # 56 x 28 x 28\n",
    "        \n",
    "                    nn.Conv2d(56, 7, (3, 3), stride=(1,1), padding=(1,1)),  # 7 x 28 x 28\n",
    "                    nn.Flatten()  # 5488 x 1 x 1\n",
    "                )\n",
    "                 \n",
    "                self.decoder = nn.Sequential(    \n",
    "                    View([-1, 7, 28, 28]),\n",
    "                    nn.ConvTranspose2d(7, 56, (3, 3), stride=(1,1), padding=(1,1)),  # 32 x 32 x 32\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Upsample(scale_factor=2),  # 32 x 64 x 64\n",
    "                \n",
    "                    nn.ConvTranspose2d(56, 112, (3,3), stride=(1,1), padding=(1,1)),  # 64 x 64 x 64\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Upsample(scale_factor=2),  # 64 x 128 x 128\n",
    "        \n",
    "                    nn.ConvTranspose2d(112, 224, (3,3), stride=(1,1), padding=(1,1)),  # 128 x 128 x 128\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Upsample(scale_factor=2),  # 64 x 128 x 128\n",
    "        \n",
    "                    nn.ConvTranspose2d(224, 1, (3,3), stride=(1,1), padding=(1,1)),\n",
    "                    nn.Sigmoid()\n",
    "                )\n",
    "        self.fc_mu = nn.Linear(self.enc_out_dim, self.latent_dim)\n",
    "        self.fc_var = nn.Linear(self.enc_out_dim, self.latent_dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def pretrained_weights_available():\n",
    "        return list(VAE.pretrained_urls.keys())\n",
    "\n",
    "    def from_pretrained(self, checkpoint_name):\n",
    "        if checkpoint_name not in VAE.pretrained_urls:\n",
    "            raise KeyError(str(checkpoint_name) + \" not present in pretrained weights.\")\n",
    "\n",
    "        return self.load_from_checkpoint(VAE.pretrained_urls[checkpoint_name], strict=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        p, q, z = self.sample(mu, log_var)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def _run_step(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        p, q, z = self.sample(mu, log_var)\n",
    "        return z, self.decoder(z), p, q\n",
    "    \n",
    "    def sample(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "        return p, q, z\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        z, x_hat, p, q = self._run_step(x)\n",
    "        ls = nn.BCELoss()\n",
    "        recon_loss = ls(x_hat, x)\n",
    "        kl = torch.distributions.kl_divergence(q, p)\n",
    "        kl = kl.mean()\n",
    "        kl *= self.kl_coeff\n",
    "\n",
    "        loss = recon_loss + kl \n",
    "\n",
    "        logs = {\n",
    "            \"recon_loss\": recon_loss,\n",
    "            \"kl\": kl,\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "        return loss, logs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logs = self.step(batch, batch_idx)\n",
    "        self.log_dict({f\"train_{k}\": v for k, v in logs.items()}, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logs = self.step(batch, batch_idx)\n",
    "        self.log_dict({f\"val_{k}\": v for k, v in logs.items()})\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "crop_size = 224\n",
    "num_of_channels = 1\n",
    "path_to_data = \"images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataloader = create_dataset(\n",
    "    path_to_data, batch_size, crop_size, num_of_channels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(pl.Callback):\n",
    "    def __init__(self, input_imgs, run_id=\"\", every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "        # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "        self.save_path = \"Results_VAE\\\\{r}\\\\\".format(r=run_id)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module, save_to=\"Results_VAE/\"):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Reconstruct images\n",
    "            save_to = self.save_path + \"epoch_{e}\\\\\".format(e=trainer.current_epoch)\n",
    "            input_imgs = self.input_imgs.to(pl_module.device)\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                reconst_imgs = pl_module(input_imgs)\n",
    "                pl_module.train()\n",
    "            # Plot and add to tensorboard\n",
    "            imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "            grid = torchvision.utils.make_grid(\n",
    "                imgs, nrow=2, normalize=True, range=(-1, 1)\n",
    "            )\n",
    "            trainer.logger.experiment.add_image(\n",
    "                \"Reconstructions\", grid, global_step=trainer.global_step\n",
    "            )\n",
    "            for i in range(len(input_imgs)):\n",
    "                pyplot.figure()\n",
    "                pyplot.subplot(121)\n",
    "                pyplot.imshow(input_imgs[i].cpu().detach().numpy()[0], cmap=\"gray\")\n",
    "                pyplot.title(\"original\")\n",
    "                pyplot.subplot(122)\n",
    "                pyplot.imshow(reconst_imgs[i].cpu().detach().numpy()[0], cmap=\"gray\")\n",
    "                pyplot.title(\"reconstruction\")\n",
    "\n",
    "                if save_to is not None:\n",
    "                    if not os.path.exists(save_to + \"Images/\"):\n",
    "                        os.makedirs(save_to + \"Images/\")\n",
    "                    pyplot.savefig(save_to + \"Images/{}.pdf\".format(i))\n",
    "                else:\n",
    "                    pyplot.show()\n",
    "            pyplot.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_images(num, rand=True):\n",
    "    if rand == True:\n",
    "        return torch.stack(\n",
    "            [dataset[i][0] for i in random.sample(range(0, len(dataset)), num)], dim=0\n",
    "        )\n",
    "    else:\n",
    "        return torch.stack([dataset[i][0] for i in range(num)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_AE(run_id):\n",
    "    vae = VAE(crop_size, lr=1e-3, kl_coeff=0.05)\n",
    "    logger = TensorBoardLogger(\"Results_VAE\", name=run_id)\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        max_epochs=300,\n",
    "        callbacks=[\n",
    "            GenerateCallback(get_train_images(8), every_n_epochs=1, run_id=run_id)\n",
    "        ],\n",
    "        log_every_n_steps=5,\n",
    "    )\n",
    "    return trainer, vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer, ae = Train_AE(\"Results_VAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\waren\\mambaforge\\envs\\cc\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 288 K \n",
      "1 | decoder | Sequential | 288 K \n",
      "2 | fc_mu   | Linear     | 30.1 M\n",
      "3 | fc_var  | Linear     | 30.1 M\n",
      "---------------------------------------\n",
      "60.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.8 M    Total params\n",
      "243.294   Total estimated model params size (MB)\n",
      "c:\\Users\\waren\\mambaforge\\envs\\cc\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6efe052c83464481866535f2b53668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.fit(ae, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
